{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard test notebook\n",
    "Here is the code to create a simple model visualisation with TensorBoard. This one uses the automatic logging features that are are included when using anything related to TensorFLow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The requirements:\n",
    "\n",
    "%pip install tensorboard tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation \n",
    "from keras.utils import np_utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating the data into training and testing sets\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# All values are between 0 and 255, dividing them to have values from 0 to 1:\n",
    "X_train_divided = X_train / 255\n",
    "X_test_divided = X_test / 255\n",
    "\n",
    "# Setting a few lists of variables to make varying the model output easy. \n",
    "first_layer_nodes = [100, 50, 100, 50, 100, 50]\n",
    "first_layer_activation = [\"relu\", \"sigmoid\", \"softmax\", \"relu\", \"sigmoid\", \"softmax\"]\n",
    "second_layer_nodes = [10, 15, 25, 25, 15, 10]\n",
    "second_layer_activation = [\"sigmoid\", \"softmax\", \"relu\", \"relu\", \"sigmoid\", \"softmax\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since TensorBoard creates a log file to visualise metrics and logged information, it's a good idea to clear previous log files, especially when running the same tests multiple times.\n",
    "Uncomment if you need to clear previous logs (the folder name is defined at the start of the run logging, in the code cell after):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf ./TensorBoard_test_logs/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to define the directory and name for each logged run, otherwise they will overwrite eachother. As you can see, I did it by iterating over a loop and using that to name my runs, but this can be done in other ways too, like manually or automatically using the \"datetime\" library. Personally, I find using datetime nice so here's a way to create the logging directory that way:\n",
    "\n",
    "%pip install datetime \\\n",
    "import datetime \\\n",
    "log_dir = 'TensorBoard_test_logs/fit' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "Something else to consider is that in this case, the model is madde using keras (from TensorFlow, that also created TensorBoard), so automatic logging is available and used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2703 - accuracy: 0.9238 - val_loss: 0.1450 - val_accuracy: 0.9573\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1223 - accuracy: 0.9645 - val_loss: 0.1116 - val_accuracy: 0.9678\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0863 - accuracy: 0.9740 - val_loss: 0.0928 - val_accuracy: 0.9735\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0657 - accuracy: 0.9802 - val_loss: 0.0909 - val_accuracy: 0.9719\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0515 - accuracy: 0.9842 - val_loss: 0.0821 - val_accuracy: 0.9750\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.5171 - accuracy: 0.8788 - val_loss: 0.2658 - val_accuracy: 0.9266\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2329 - accuracy: 0.9345 - val_loss: 0.2011 - val_accuracy: 0.9432\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1839 - accuracy: 0.9475 - val_loss: 0.1707 - val_accuracy: 0.9508\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1552 - accuracy: 0.9559 - val_loss: 0.1528 - val_accuracy: 0.9566\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1347 - accuracy: 0.9617 - val_loss: 0.1388 - val_accuracy: 0.9577\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 3.1498 - accuracy: 0.1183 - val_loss: 3.2181 - val_accuracy: 0.0984\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 3.2176 - accuracy: 0.0988 - val_loss: 3.2176 - val_accuracy: 0.0981\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 3.2159 - accuracy: 0.0988 - val_loss: 3.2140 - val_accuracy: 0.0983\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 3.1756 - accuracy: 0.0988 - val_loss: 3.1068 - val_accuracy: 0.0981\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 3.1084 - accuracy: 0.0987 - val_loss: 3.1358 - val_accuracy: 0.0980\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 2.7566 - accuracy: 0.3144 - val_loss: 3.1010 - val_accuracy: 0.1358\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 3.0965 - accuracy: 0.1352 - val_loss: 3.1285 - val_accuracy: 0.1205\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 3.1130 - accuracy: 0.1268 - val_loss: 3.1171 - val_accuracy: 0.1303\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 3.0525 - accuracy: 0.1515 - val_loss: 3.0442 - val_accuracy: 0.1530\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 3.1028 - accuracy: 0.1356 - val_loss: 3.1114 - val_accuracy: 0.1318\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.4249 - accuracy: 0.8924 - val_loss: 0.2324 - val_accuracy: 0.9354\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2028 - accuracy: 0.9418 - val_loss: 0.1733 - val_accuracy: 0.9485\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1519 - accuracy: 0.9563 - val_loss: 0.1423 - val_accuracy: 0.9585\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1202 - accuracy: 0.9652 - val_loss: 0.1155 - val_accuracy: 0.9664\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0987 - accuracy: 0.9723 - val_loss: 0.1042 - val_accuracy: 0.9686\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 1.2612 - accuracy: 0.7668 - val_loss: 0.7344 - val_accuracy: 0.8284\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.5862 - accuracy: 0.8607 - val_loss: 0.4627 - val_accuracy: 0.8973\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.4232 - accuracy: 0.9006 - val_loss: 0.3755 - val_accuracy: 0.9136\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.3535 - accuracy: 0.9157 - val_loss: 0.3364 - val_accuracy: 0.9165\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.3162 - accuracy: 0.9229 - val_loss: 0.3139 - val_accuracy: 0.9218\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    # Naming the runs and saving them in the same directory, so that we can upload the whole directory later.\n",
    "    log_dir = \"TensorBoard_test_logs/run_{}\".format(i+1)\n",
    "    tensorboard_callback= tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    # Model creation, compilation and training\n",
    "    model = Sequential([\n",
    "        keras.layers.Flatten(input_shape = (28,28), name = 'layers_flatten'),\n",
    "        keras.layers.Dense(first_layer_nodes[i], first_layer_activation[i], name = 'layers_dense_1'),\n",
    "        keras.layers.Dense(second_layer_nodes[i], second_layer_activation[i], name = 'layers_dense_2')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer = 'adam',\n",
    "        loss = 'sparse_categorical_crossentropy',\n",
    "        metrics = ['accuracy']\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train_divided,\n",
    "        y_train, \n",
    "        epochs = 5, \n",
    "        validation_data = (X_test_divided, y_test), \n",
    "        callbacks = [tensorboard_callback]\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualise logged information, run the following cell, specifying the log directory. This will serve TensorBoard on localhost, the link to which should be provided in the ouput (something like http://localhost:6006). Remember that if you stop the cell, you are stopping the localhost connection and so won't be able to load any new graphs for visualisation that you haven't already opened.\n",
    "\n",
    "It is also possible to serve the logs to the tensorboard.dev service, for sharing purposes. You can optionally specify some parameters (metadata), like in the commented out example the name of the uploaded logs, and a short description. To see the version of this test that I already uploaded to tensorboard.dev, go to https://tensorboard.dev/experiment/foEbSji9RVuQlhleSXGb1A/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.12.3 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir TensorBoard_test_logs/\n",
    "\n",
    "# !tensorboard dev upload \\\n",
    "# --logdir TensorBoard_test_logs/ \\\n",
    "# --name \"Visualising 6 models with different CNN layers\" \\\n",
    "# --description \"TensorBoard default logs for 6 MNIST digit recognition models, with varying performance according to layer nodes and layer activation.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of TensorBoard, unique featrues include the automatic logging of tensors from the keras model's training layers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
