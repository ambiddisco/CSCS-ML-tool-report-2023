{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights & Biases test notebook\n",
    "This example notebook shows the code I used to create the example graphs shown on the report for Weights & Biases. \\\n",
    "Please remember that to run this and create your own graphs, you need an account on the W&B web page, at https://wandb.ai/site. \\\n",
    "Remember also that to view the visualisations and access the model registry, you need to run \"wandb server\" in a terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The requirements:\n",
    "\n",
    "%pip install wandb tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The MNIST classifier dataset is a set of images, made up of 28x28 pixels, of value between 0 and 255.\n",
    "# First of all we need to split the 70'000 images into a training set and a testing one:\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# For simpler use of the data, we want each value to be between 0 and 1.\n",
    "X_train_divided = X_train / 255\n",
    "X_test_divided = X_test / 255\n",
    "\n",
    "# This next block is a simple way to create models with varying accuracy and loss.\n",
    "first_layer_nodes = [100, 50, 100, 50, 100, 50]\n",
    "first_layer_activation = [\"relu\", \"sigmoid\", \"softmax\", \"relu\", \"sigmoid\", \"softmax\"]\n",
    "second_layer_nodes = [10, 15, 25, 25, 15, 10]\n",
    "second_layer_activation = [\"sigmoid\", \"softmax\", \"relu\", \"relu\", \"sigmoid\", \"softmax\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before logging in to wandb, it's necessary to create an account on the Weghts & Biases website, at https://wandb.ai/site. I personally used my Google account and it worked fine for SaaS testing, it's slightly simpler, whereas for the local server a licence may be required. \\\n",
    "If it's the first time logging into your account, you should log in on the website, navigate to https://wandb.ai/authorize, run \"wandb.login()\", copy your API key and paste it under the login when prompted. \\\n",
    "Alternatively, you can log in directly with \"wandb.login(key='your API key')\", with the key (https://wandb.ai/authorize) as a string in quotes.\n",
    "\n",
    "If you get errors at the login stage check that the wandb server is still running in the terminal, or if you didn't start it yet, run \"wandb server\" in a terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next code cell we will log the metrics for the 6 different model compositions. We will be logging metadata such as the hyperparameters for each run, which we will name \"run_1\" through \"run_6\", and the metrics accuracy and loss. Some data is automatically logged, such as GPU usage throughout the run (in my case no GPUs were used, sorry to disappoint, but CPU usage is logged instead)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run_number in range(6):\n",
    "    # Each run logged with wandb should be initialised and then ended to close the connection with the server.\n",
    "    wandb.init(project = \"MNIST model variations\",\n",
    "               config = { # these are all hyperparameters\n",
    "                   \"architecture\": \"CNN\",\n",
    "                   \"dataset\":  \"keras MNIST dataset\",\n",
    "                   \"first layer nodes\": first_layer_nodes[run_number],\n",
    "                   \"first layer activation\": first_layer_activation[run_number],\n",
    "                   \"second layer nodes\": second_layer_nodes[run_number],\n",
    "                   \"second layer activation\": second_layer_activation[run_number],\n",
    "                   \"optimizer\": \"adam\",\n",
    "                   \"loss calculation\": \"sparse_categorical_crossentropy\",\n",
    "                   \"epochs\": 5,\n",
    "               },\n",
    "               name = \"run_{}\".format(run_number + 1))\n",
    "    \n",
    "    # Create, compile and train the model:\n",
    "    model = Sequential([\n",
    "        keras.layers.Flatten(input_shape = (28,28)),\n",
    "        keras.layers.Dense(first_layer_nodes[run_number], first_layer_activation[run_number]),\n",
    "        keras.layers.Dense(second_layer_nodes[run_number], second_layer_activation[run_number])\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer = 'adam',\n",
    "        loss = 'sparse_categorical_crossentropy',\n",
    "        metrics = ['accuracy']\n",
    "    )\n",
    "    history = model.fit(X_train_divided, y_train, epochs = 5, verbose = 1)\n",
    "\n",
    "    # For each run we want to log the accuracy and loss after each epoch.\n",
    "    for i in range(5):\n",
    "         wandb.log({\"acc\": history.history['accuracy'][i], \"loss\": history.history['loss'][i]})\n",
    "    \n",
    "    wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of Weights & Biases, unique features include automatic hardware usage logging, such as the CPU usage, or logging of usage of each CPU core."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
